---
# General
model_sel: "qgnn" # qmlp, qftgnn, qgnn
post_model_sel: "pqgnn" # only for split training
n_momenta: 4
default_modes: ["train", "val"]
validation_mode: ["val"]

# Classical
epochs: 10
normalize: "smartone" # one, smartone, zmuv
normalize_individually: False
zero_mean: False
dim_feedforward: 32
dim_feedforward_range: [4, 2048, "log"]
batchnorm: True

dropout_rate: 0.2
dropout_rate_range: [0.1, 1.0, "linear"]
learning_rate: 0.01
learning_rate_range: [1e-5, 1e-1, "log"]
learning_rate_decay: 100
learning_rate_decay_range: [1, 1000, "log"]
gamma: 0.5
batch_size: 8
batch_size_range: [1, 64, "log"]
gradients_clamp: 1000
gradients_spreader: 1e-10
skip_block: True # activates skip connections on local scales
skip_global: True # activates skip connections on global scales

# GNN
n_blocks: 3
n_blocks_range: [1, 5, "linear"]
n_layers_mlp: 2 # initial mlp layers before the blocks
n_layers_mlp_range: [1, 5, "linear"]
n_additional_mlp_layers: 0 # additional layers within a block
n_additional_mlp_layers_range: [1, 5, "linear"]
n_final_mlp_layers: 2 # layers after the blocks
n_final_mlp_layers_range: [1, 5, "linear"]
factor: True
tokenize: -1 # currently not supported as findings in baumbauen did not show any advantage
embedding_dims: -1 # no effect if tokenization not enabled
symmetrize: True

# Quantum
data_reupload: True
data_reupload_range: [True, False]
add_rot_gates: True
add_rot_gates_range: [True, False]
n_layers_vqc: 3
n_layers_vqc_range: [1, 5, "linear"]
padding_dropout: True
measurement: "all" # mutually_exclusive, all, entangled
backend: "aer_simulator_statevector" # aer_simulator_statevector, perth
#TODO specify when available
redis_host: "localhost"
redis_port: 6379
redis_path: "partiqlegan"
redis_password: ""

# PyTorch
detect_anomaly: False # enables torch' anomaly detector
device: 'cpu' # anything but 'cpu' to use cuda devices instead

# MLFLOW
plot_mode: "val"
plotting_rows: 4
log_gradients: False # set false when using models where the quantum layer is not the first one
git_hash_identifier: "git_hash"