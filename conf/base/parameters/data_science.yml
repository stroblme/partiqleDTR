---
# General
model_sel: "gnn" # qmlp, qftgnn, qgnn
post_model_sel: "pqgnn" # only for split training
n_momenta: 4
default_modes: ["train", "val"]
validation_mode: ["val"]

# Classical
epochs: 20
normalize: "smartone" # one, smartone, zmuv
normalize_individually: False
zero_mean: False
dim_feedforward: 32 #32
dim_feedforward_range: [8, 16, 32, 64] # must be an even number
batchnorm: True

dropout_rate: 0.2 #0.2
classical_optimizer: "Adam"
dropout_rate_range: [0.05, 0.5, "linear"]
learning_rate: 0.001 #0.001
learning_rate_range: [0.001, 0.1, "log"]
learning_rate_decay: 0.5 #100
learning_rate_decay_range: [0.01, 1.0, "log"]
decay_after: 10
batch_size: 8 #8
batch_size_range: [1, 2, 4, 8, 16, 32]
gradients_clamp: 1000
gradients_spreader: 1e-10

# GNN
n_blocks: 3 #3
n_blocks_range: [1, 5, "linear"]

n_layers_mlp: 2 # initial mlp layers before the blocks
n_layers_mlp_range: [1, 5, "linear"]

n_additional_mlp_layers: 0 #0 # additional layers within a block
n_additional_mlp_layers_range: [1, 5, "linear"]

n_final_mlp_layers: 2 #2 # layers after the blocks
n_final_mlp_layers_range: [1, 5, "linear"]

symmetrize: True
skip_block: True # activates skip connections on local scales
skip_global: True # activates skip connections on global scales

# Quantum
data_reupload: False
data_reupload_range_quant: [True, False]
add_rot_gates: True
add_rot_gates_range_quant: [True, False]

n_shots: 1024
n_shots_range_quant: [16, 64, 256, 1024, 2048]

n_layers_vqc: 2
n_layers_vqc_range_quant: [1, 3, "linear"]
gradient_curvature_threshold: 1e-10
gradient_curvature_threshold_range_quant: [1e-12, 1e-11, 1e-10, 1e-9]
gradient_curvature_history: 5  # set to 0 to disable gradient curvature/ parameter pruning
gradient_curvature_history_range_quant: [0, 2, 4, 6, 8]  # set to 0 to disable gradient curvature/ parameter pruning
initialization_constant: 1 # this scales the interval [-a pi .. a pi]
# initialization_constant_range_quant: [0.1, 1.0, "linear"]  # this scales the interval [-a pi .. a pi]
initialization_offset: 0 # this offsets the with a factor pi
# initialization_offset_range_quant: [0, 0.5, 1, 2] # this offsets the with a factor pi
parameter_seed: 1111
padding_dropout: False

predefined_vqc: "circuit_19" # "", "circuit_191", "circuit_192", "circuit_19", .. tba.
predefined_vqc_range_quant: ["circuit_191", "circuit_19", "circuit_18", "circuit_17", "circuit_16"]

predefined_iec: "direct_mapping" 
measurement: "entangled" # mutually_exclusive, all, entangled
backend: "aer_simulator" # aer_simulator, aer_simulator_statevector, ibm_perth, fake_ibm_perth
quantum_optimizer: "Adam" #SGD, Adam
quantum_momentum: #0.9 #0.001
quantum_learning_rate: 0.01 #0.001
quantum_learning_rate_range_quant: [0.001, 0.1, "log"]
quantum_learning_rate_decay: 0.5 #100
quantum_learning_rate_decay_range_quant: [0.01, 1.0, "log"]

# Optuna
n_trials: 20
timeout: 10800 #30h
optuna_path: "studies/partiqledtr.db"
optuna_sampler_seed:  # should be None, if n_jobs=1 and separate processes are triggered from console
selective_optimization: False # if true, only optimize classical params
resume_study: True
n_jobs: 1
run_id: "OptunaOptimization#008"

pool_process: True # alternative pool processing
pruner_startup_trials: 10
pruner_warmup_steps: 5
pruner_interval_steps: 1
pruner_min_trials: 10

# PyTorch
detect_anomaly: True # enables torch' anomaly detector
device: 'cpu' # anything but 'cpu' to use cuda devices instead
torch_seed: 1111

# MLFLOW
plot_mode: "val"
plotting_rows: 4
log_gradients: True # set false when using models where the quantum layer is not the first one
git_hash_identifier: "git_hash"